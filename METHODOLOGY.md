# METHODOLOGY: Data Pipeline for Aadhaar System Analysis

## Overview

To understand how we analyze the vast amount of data generated by the Aadhaar system, we follow a structured, step-by-step methodology. Think of this as a **"data pipeline"** that takes raw, messy information and transforms it into a clear roadmap for government decision-makers. This comprehensive approach combines state-level national analysis with granular pincode-level examination of high-risk regions.

---

## PART 1: NATIONAL-LEVEL ANALYSIS (State & Temporal Aggregation)

### 1. Data Harvesting and Consolidation

**Purpose**: The Aadhaar system generates data across different categories—biometrics (fingerprints/iris scans), demographics (names/addresses), and total enrollments. These are often stored as hundreds of separate files across different folders.

**Process**:
- We implement a specialized **folder-scanning function** that automatically traverses data directories and identifies all CSV files
- The function reads each file sequentially and concatenates them using pandas' `pd.concat()` with `ignore_index=True` to maintain data continuity
- Three separate pipelines create unified master datasets:
  - **Enrollment Master**: Consolidates all enrollment records from biometric, demographic, and enrollment data folders
  - **Biometric Master**: Aggregates all successful biometric authentication records
  - **Demographic Master**: Consolidates all demographic data collection records

**Result**: Instead of analyzing fragmented files, we obtain a unified view of the entire system's activity. The datasets contain millions of records spanning multiple states and months, ensuring no data point is left behind during the analysis.

**Code Implementation**:
```python
def load_folder(folder_path):
    dfs = []
    for file in os.listdir(folder_path):
        if file.endswith(".csv"):
            dfs.append(pd.read_csv(os.path.join(folder_path, file)))
    return pd.concat(dfs, ignore_index=True)

bio_df    = load_folder("api_data_aadhar_biometric")
demo_df   = load_folder("api_data_aadhar_demographic")
enroll_df = load_folder("api_data_aadhar_enrolment")
```

---

### 2. Standardization and Data Cleaning

**Purpose**: Human error is endemic in large-scale data collection. For example:
- One record might say "WestBengal" while another says "West Bengal"
- Odisha may be recorded as "Orissa" in older entries
- Inconsistent spacing and capitalization across the database
- Date formats may vary (DD/MM/YYYY vs MM/DD/YYYY)

Computers see these as completely different values, which breaks mathematical analysis.

**Process**:
1. **Column Name Normalization**: Strip whitespace from column headers using `.str.strip()`
2. **State Name Standardization**: 
   - Convert all text to lowercase using `.str.lower()`
   - Strip leading/trailing whitespace
   - Apply targeted replacements for common variations:
     - "westbengal" → "west bengal"
     - "tamilnadu" → "tamil nadu"
     - "orissa" → "odisha"
     - "chhatisgarh" → "chhattisgarh"
     - "dadra & nagar haveli" → "dadra and nagar haveli"
3. **Date Parsing**: Convert text-based dates to datetime objects using `pd.to_datetime()` with `dayfirst=True` to handle Indian date formats
4. **Temporal Feature Engineering**:
   - Extract **month** as a period object (Year-Month) for monthly aggregation
   - Extract **year** as integer for annual analysis
   - These enable time-series analysis and seasonal pattern detection
5. **Data Structure Optimization**: Remove unnecessary columns (e.g., pincode) at this stage when not needed for state-level analysis

**Result**: A clean, standardized dataset where every state and every month is labeled identically. This consistency is foundational for accurate mathematical operations and comparative analysis across the country.

**Code Implementation**:
```python
def normalize_and_preprocess(df):
    # Clean column names
    df.columns = df.columns.str.strip()
    
    # Normalize state
    df['state'] = (
        df['state']
        .str.strip()
        .str.lower()
        .replace({
            'westbengal': 'west bengal',
            'tamilnadu': 'tamil nadu',
            'orissa': 'odisha',
            'dadra & nagar haveli': 'dadra and nagar haveli',
            'chhatisgarh': 'chhattisgarh',
        })
    )
    
    # Parse date
    df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')
    df['month'] = df['date'].dt.to_period("M")
    df['year']  = df['date'].dt.year
    
    return df

enroll_df = normalize_and_preprocess(enroll_df)
demo_df   = normalize_and_preprocess(demo_df)
bio_df    = normalize_and_preprocess(bio_df)
```

---

### 3. Data Aggregation by Geographic and Temporal Dimensions

**Purpose**: Raw records show individual transactions. To understand system-wide performance, we need to aggregate data into meaningful units—months and states.

**Process**:
- Group enrollment, demographic, and biometric records by **['month', 'state']** using pandas `.groupby()`
- Apply `.sum(numeric_only=True)` to aggregate all numeric columns (age groups, counts, etc.)
- Create three aggregated datasets that align on month and state dimensions
- Standardize column names across datasets for easy merging:
  - Enrollment: 'age_5_17' → 'enroll_5_17', 'age_18_greater' → 'enroll_18p'
  - Demographics: 'demo_age_5_17' → 'demo_5_17', 'demo_age_17_' → 'demo_18p'
  - Biometrics: 'bio_age_5_17' → 'bio_5_17', 'bio_age_17_' → 'bio_18p'

**Result**: Three monthly state-level aggregated datasets ready for merging and comparison.

**Code Implementation**:
```python
enroll_agg = enroll_df.groupby(['month','state'], as_index=False).sum(numeric_only=True)
demo_agg   = demo_df.groupby(['month','state'], as_index=False).sum(numeric_only=True)
bio_agg    = bio_df.groupby(['month','state'], as_index=False).sum(numeric_only=True)

# Rename for clarity
enroll_agg = enroll_agg.rename(columns={
    'age_5_17': 'enroll_5_17',
    'age_18_greater': 'enroll_18p'
})

bio_agg = bio_agg.rename(columns={
    'bio_age_5_17': 'bio_5_17',
    'bio_age_17_': 'bio_18p'
})
```

---

### 4. Creating Success Metrics (Completion Rates)

**Purpose**: Simply looking at raw numbers is misleading. A state might have 1,000,000 enrollments, but if only 10,000 people successfully gave their fingerprints, the system is functionally failing. We need a **normalized metric** that accounts for effort versus results.

**Process**:
1. **Merge Datasets**: Combine enrollment, demographic, and biometric aggregates on `['month', 'state']`
2. **Handle Zero Values**: Replace 0 enrollments with `np.nan` to avoid division-by-zero errors
3. **Calculate Completion Rates**:
   - Demographic Completion Rate = (Demographics Collected) / (Total Enrollments)
   - Biometric Completion Rate = (Biometric Records) / (Total Enrollments)
   - Calculated separately for two age groups: 5-17 years and 18+ years
4. **Data Validation**:
   - Replace infinite values (∞ and -∞) with `np.nan` to handle edge cases
   - These emerge when dividing by zero despite preventive measures

**Result**: A clear "efficiency score" (0 to 1, where 1 is 100% completion) for every state and month. This tells us exactly how successful the data collection process is, independent of population size.

**Code Implementation**:
```python
df = (
    enroll_agg
    .merge(demo_agg[['month','state','demo_5_17','demo_18p']], 
           on=['month','state'], how='left')
    .merge(bio_agg[['month','state','bio_5_17','bio_18p']], 
           on=['month','state'], how='left')
)

# Replace zeros with NaN to prevent division errors
df[['enroll_18p','enroll_5_17']] = df[['enroll_18p','enroll_5_17']].replace(0, np.nan)

# Calculate completion rates
df['demo_rate_18p'] = df['demo_18p'] / df['enroll_18p']
df['bio_rate_18p']  = df['bio_18p']  / df['enroll_18p']

df['demo_rate_5_17'] = df['demo_5_17'] / df['enroll_5_17']
df['bio_rate_5_17']  = df['bio_5_17']  / df['enroll_5_17']

# Handle edge cases
df.replace([np.inf, -np.inf], np.nan, inplace=True)
```

---

### 5. Exploring Data "Shape" (Distribution Analysis)

**Purpose**: In large populations, data is often **skewed**. A few high-performing areas can make the whole country look good, hiding thousands of smaller areas that are struggling. We need to understand the typical experience versus the extreme experience.

**Process**:
1. **Visual Inspection** using four complementary plots:
   - **Histogram with KDE**: Reveals the overall shape of the distribution and identifies modality (single peak vs. multiple peaks)
   - **Boxplot**: Shows quartiles, median, and outliers; highlights the interquartile range containing 50% of data
   - **Log-Transformed Histogram**: Applies logarithmic transformation to reveal patterns in highly skewed distributions; useful for detecting power-law or log-normal behavior

2. **Statistical Tests**:
   - **Skewness**: Measures asymmetry of the distribution
     - Positive skewness: Right tail (more extreme high values)
     - Negative skewness: Left tail (more extreme low values)
   - **Kurtosis**: Measures the "heaviness" of tails
     - High kurtosis: Heavy tails with extreme outliers
     - Low kurtosis: Lighter tails, fewer extremes

3. **Data Cleaning for Analysis**:
   - Remove NaN and infinite values before calculating statistical measures
   - Use `replace([np.inf, -np.inf], np.nan).dropna()` to ensure valid input

**Result**: A deep understanding of the typical performance (median, IQR) versus extreme cases (outliers, heavy tails). This helps identify hidden patterns in how different regions perform.

**Code Implementation**:
```python
# Data cleaning
data_clean = df['bio_rate_18p'].replace([np.inf, -np.inf], np.nan).dropna()

# Statistical analysis
print("Skewness:", skew(data_clean))
print("Kurtosis:", kurtosis(data_clean))

# Visualization
plt.figure(figsize=(18,6))

plt.subplot(1,3,1)
sns.histplot(data, bins=30, kde=True, color='skyblue')
plt.title('Histogram & KDE of bio_rate_18p')

plt.subplot(1,3,2)
sns.boxplot(x=data, color='lightgreen')
plt.title('Boxplot of bio_rate_18p')

plt.subplot(1,3,3)
sns.histplot(np.log1p(data), bins=30, kde=True, color='salmon')
plt.title('Log-Transformed Histogram of bio_rate_18p')

plt.tight_layout()
plt.show()
```

---

### 6. State-Level Performance Benchmarking

**Purpose**: Identify which states are performing well and which need urgent intervention.

**Process**:
1. Group the merged dataset by `'state'` and calculate mean biometric completion rate
2. Sort states from lowest to highest performance
3. Visualize using a **log-scale horizontal bar chart** to accommodate the wide range of values
4. The logarithmic scale is critical because:
   - Some states may have rates of 0.01 (1%)
   - Others have rates of 10 (1000%)
   - Without log scale, high performers dominate the visual and low performers become invisible

**Result**: Clear identification of which states are critical priorities for intervention.

**Code Implementation**:
```python
state_avg = df.groupby('state')['bio_rate_18p'].mean().sort_values()

plt.figure(figsize=(12, 10))
plt.barh(state_avg.index, state_avg.values)
plt.xscale('log')

plt.title("Average Biometric Completion Rate (18+) [Log Scale]", fontsize=16)
plt.xlabel("Biometric Rate (log scale)", fontsize=14)
plt.ylabel("State", fontsize=14)

plt.tight_layout()
plt.savefig("avg_biometric_completion_rate_18plus_log.png", dpi=300)
```

---

## PART 2: PINCODE-LEVEL ANALYSIS (Granular Geographic Focus)

### 7. Strategic Focus on High-Risk States

**Purpose**: Not all regions warrant the same level of detailed analysis. Seven states have been identified as requiring urgent intervention based on national-level findings.

**Risk States Included**:
- Assam
- Dadra and Nagar Haveli and Daman and Diu
- Goa
- Ladakh
- Meghalaya
- Mizoram
- Sikkim

**Process**:
1. Filter biometric and enrollment master datasets to include only records from these seven states
2. This reduces dataset size while maintaining focus on areas of highest concern
3. All subsequent pincode-level analysis applies only to these states

**Result**: A focused dataset containing biometric and enrollment records from 7 high-risk states only.

**Code Implementation**:
```python
RISK_STATES = [
    "assam",
    "dadra and nagar haveli and daman and diu",
    "goa",
    "ladakh",
    "meghalaya",
    "mizoram",
    "sikkim"
]

bio_df    = bio_df[bio_df["state"].isin(RISK_STATES)]
enroll_df = enroll_df[enroll_df["state"].isin(RISK_STATES)]
```

---

### 8. Granular Aggregation: Pincode-Level Analysis

**Purpose**: While state-level analysis shows overall trends, pincodes are the operational frontline. Each pincode represents a specific geographic locality with its own enrollment center, internet connectivity, staff capacity, and population dynamics. Analyzing at pincode level reveals where specific problems exist.

**Process**:
1. **Hierarchical Aggregation**: Group biometric and enrollment data by `['month', 'state', 'district', 'pincode']`
2. This creates the finest granular level in our dataset—all records from the same month, same pincode aggregated together
3. Apply `.sum(numeric_only=True)` to combine counts and metrics
4. Standardize column names:
   - Biometric: 'bio_age_17_' → 'bio_18p'
   - Enrollment: 'age_18_greater' → 'enroll_18p'
5. **Merge on Fine Granularity**: Merge biometric and enrollment aggregates on `['month', 'state', 'pincode']` (note: district is used for joining but not in final merge key)

**Result**: Monthly pincode-level data showing biometric and enrollment counts for the 18+ age group—the primary focus of Aadhaar biometric collection.

**Code Implementation**:
```python
bio_agg = (
    bio_df
    .groupby(["month", "state", "district", "pincode"], as_index=False)
    .sum(numeric_only=True)
)

enroll_agg = (
    enroll_df
    .groupby(["month", "state", "district", "pincode"], as_index=False)
    .sum(numeric_only=True)
)

bio_agg = bio_agg.rename(columns={"bio_age_17_": "bio_18p"})
enroll_agg = enroll_agg.rename(columns={"age_18_greater": "enroll_18p"})

# Merge on finest granularity
df = bio_agg.merge(
    enroll_agg[["month", "state", "pincode", "enroll_18p"]],
    on=["month", "state", "pincode"],
    how="left"
)
```

---

### 9. Pincode-Level Completion Rate Calculation

**Purpose**: Calculate biometric completion rate at the pincode-month level to identify exactly where and when the system is failing.

**Process**:
1. Replace zero enrollments with `np.nan` to prevent mathematical errors
2. Calculate **Biometric Completion Rate** = (Biometric Records) / (Enrollments) for 18+ age group
3. Handle edge cases:
   - Replace infinite and negative infinite values with NaN
   - Drop rows where completion rate could not be calculated (missing enrollment data)
4. Result: A clean dataset with valid completion rates for each pincode-month combination

**Result**: Every pincode-month has a clear completion rate (0-1, where 1 = 100% completion).

**Code Implementation**:
```python
df["enroll_18p"] = df["enroll_18p"].replace(0, np.nan)
df["bio_rate_18p"] = df["bio_18p"] / df["enroll_18p"]

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(subset=["bio_rate_18p"], inplace=True)
```

---

### 10. Anomaly Detection at Pincode-Month Level

**Purpose**: Not all failures are equal. A single month of low performance might be temporary. Systemic failures require priority. We define clear, quantitative criteria for what constitutes an "anomaly."

**Anomaly Definition**:
1. **Low-Performance Anomaly**: Completion rate falls below the 5th percentile of all pincode-month observations
   - Interpretation: Bottom 5% of all performances
   - Threshold is calculated as: `LOW_THRESHOLD = df["bio_rate_18p"].quantile(0.05)`
2. **Complete Failure Anomaly**: Completion rate equals exactly zero
   - Interpretation: Zero biometric records collected despite enrollment attempts
   - Indicates system unavailability, staff absence, or infrastructure failure

**Process**:
1. Calculate 5th percentile threshold from the entire pincode-month dataset
2. Create binary flags:
   - `low_anomaly`: True if completion rate < LOW_THRESHOLD
   - `zero_anomaly`: True if completion rate = 0
3. Combine both conditions: A record is anomalous if it meets either criterion
4. Count total anomalies for severity assessment

**Result**: A binary anomaly flag for every pincode-month record, enabling risk scoring.

**Code Implementation**:
```python
LOW_THRESHOLD = df["bio_rate_18p"].quantile(0.05)

df["low_anomaly"]  = df["bio_rate_18p"] < LOW_THRESHOLD
df["zero_anomaly"] = df["bio_rate_18p"] == 0

anomalies = df[df["low_anomaly"] | df["zero_anomaly"]]
print("Total anomalies:", anomalies.shape[0])
```

---

### 11. Pincode-Level Summary Statistics

**Purpose**: Transform monthly pincode records into pincode-level summary statistics. This enables ranking and prioritization.

**Process**:
1. Group by `['state', 'pincode']` (collapsing all months)
2. Aggregate using:
   - **avg_bio_rate**: Mean of all monthly completion rates (overall performance)
   - **anomaly_count**: Sum of low-anomaly flags (how many low-performance months)
   - **zero_count**: Sum of zero-anomaly flags (how many complete-failure months)
   - **months_observed**: Count of non-null completion rates (tenure in dataset)
3. Sort by state and average rate for easy browsing

**Result**: One row per pincode showing summary statistics across all months.

**Code Implementation**:
```python
pincode_summary = (
    df.groupby(["state", "pincode"], as_index=False)
      .agg(
          avg_bio_rate=("bio_rate_18p", "mean"),
          anomaly_count=("low_anomaly", "sum"),
          zero_count=("zero_anomaly", "sum"),
          months_observed=("bio_rate_18p", "count")
      )
      .sort_values(["state", "avg_bio_rate"])
)
```

---

### 12. Composite Risk Scoring

**Purpose**: Rank pincodes by risk. Not all failures are equal:
- A pincode that never fails but has low average rate is different from one that fails frequently
- A pincode that completely fails once is different from one that fails every month

We combine three failure dimensions into a single **Composite Risk Score**.

**Risk Score Components**:

| Component | Formula | Weight | Interpretation |
|-----------|---------|--------|-----------------|
| **Zero Ratio** | zero_count / months_observed | 50% | How often does the system completely fail? |
| **Anomaly Ratio** | anomaly_count / months_observed | 30% | How often is performance anomalously low? |
| **Low-Performance Penalty** | 1 / (1 + avg_bio_rate) | 20% | Is overall performance poor? |

**Weighting Rationale**:
- **50% Zero Ratio**: Complete system failure (zero biometric collection) is the most critical issue. It indicates infrastructure breakdown or unavailability.
- **30% Anomaly Ratio**: Frequent low performance indicates chronic under-resourcing or operational difficulties.
- **20% Low-Performance Penalty**: Even with no failures, consistently low completion rates indicate capability gaps.

**Result**: Pincodes with higher composite risk scores have more severe, more frequent, and more systemic issues.

**Code Implementation**:
```python
pincode_summary["zero_ratio"] = (
    pincode_summary["zero_count"] / pincode_summary["months_observed"]
)

pincode_summary["anomaly_ratio"] = (
    pincode_summary["anomaly_count"] / pincode_summary["months_observed"]
)

pincode_summary["risk_score"] = (
    0.5 * pincode_summary["zero_ratio"] +
    0.3 * pincode_summary["anomaly_ratio"] +
    0.2 * (1 / (1 + pincode_summary["avg_bio_rate"]))
)
```

---

### 13. Critical Pincode Identification

**Purpose**: Identify the "top 10 critical pincodes" per state that require immediate intervention.

**Process**:
1. Sort entire pincode_summary dataset by `risk_score` in descending order (highest risk first)
2. Group by `'state'`
3. Select the top 10 within each state using `.head(10)`
4. Result: Maximum 70 pincodes (10 per state × 7 states), though some may be fewer if a state has fewer than 10 pincodes

**Result**: A prioritized list of specific pincodes and districts requiring urgent operational intervention.

**Code Implementation**:
```python
CRITICAL_PINCODES = (
    pincode_summary
    .sort_values("risk_score", ascending=False)
    .groupby("state")
    .head(10)
)
```

---

### 14. District-Level Summary Statistics

**Purpose**: In addition to pincode analysis, create district-level aggregates to understand broader geographic patterns and identify district-wide challenges.

**Process**:
1. Group pincode-month data by `['state', 'district']` to collapse both states and months
2. Aggregate:
   - **avg_bio_rate**: Mean completion rate across all months and all pincodes in the district
   - **anomaly_months**: Total count of low-performance months across all pincodes
   - **zero_months**: Total count of complete-failure months across all pincodes
   - **total_months**: Total month-pincode observations in the district
3. Calculate **zero_ratio** = zero_months / total_months

**Result**: One row per district summarizing its overall performance and failure patterns.

**Code Implementation**:
```python
district_summary = (
    df.groupby(["state", "district"], as_index=False)
      .agg(
          avg_bio_rate=("bio_rate_18p", "mean"),
          anomaly_months=("low_anomaly", "sum"),
          zero_months=("zero_anomaly", "sum"),
          total_months=("bio_rate_18p", "count")
      )
)

district_summary["zero_ratio"] = (
    district_summary["zero_months"] / district_summary["total_months"]
)
```

---

### 15. Strategic Visualization and Reporting

**Purpose**: Most stakeholders cannot understand thousands of rows of numbers. Visual representations make the "story" of the data immediately apparent and actionable.

**Visualization Strategy**:

#### **State-Level Dual Histograms**

For each high-risk state, generate paired histograms:

1. **Left Panel - Zero Capture Ratio Distribution**:
   - X-axis: Proportion of months where biometric collection completely failed (0 to 1)
   - Y-axis: Number of pincodes with that zero-ratio
   - Interpretation:
     - High bars near 0: Many pincodes collecting biometrics consistently
     - High bars near 1: Many pincodes with systemic failures
     - Distribution skewed toward 1: Severe operational issues requiring intervention

2. **Right Panel - Non-Zero Biometric Rate Distribution (Log Scale)**:
   - X-axis (logarithmic): Average biometric completion rate for pincodes with non-zero rates
   - Y-axis: Number of pincodes with that rate
   - Log scale accommodates extreme range (from 0.01 to 100+)
   - Interpretation:
     - Wide spread: High variation in capacity and infrastructure across pincodes
     - Sharp peak at low values: Majority struggle with completion even when data collection occurs
     - Reveals which pincodes operate efficiently vs. struggling

**Purpose of Dual Visualization**:
- Left panel reveals **frequency of failure** (how often does collection stop?)
- Right panel reveals **severity of underperformance** (when collection happens, how well does it perform?)
- Together, they show both problems: failures AND low efficiency

**Code Implementation**:
```python
for state in RISK_STATES:
    state_df = pincode_summary[pincode_summary["state"] == state]
    
    if state_df.empty:
        continue
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Zero dominance
    axes[0].hist(state_df["zero_ratio"], bins=20)
    axes[0].set_title(f"Zero Capture Ratio — {state}")
    
    # Severity (non-zero average bio rate)
    axes[1].hist(state_df[state_df["avg_bio_rate"] > 0]["avg_bio_rate"], bins=30)
    axes[1].set_xscale("log")
    axes[1].set_title(f"Non-zero Avg Bio Rate — {state}")
    
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/{state}_pincode_histograms.png", dpi=300)
```

---

## Key Methodological Principles

### 1. **Data Integrity at Every Step**
- Validate shapes after each loading and transformation
- Check for unexpected NaN and infinite values
- Use meaningful aggregation levels (month-state-district-pincode hierarchy)

### 2. **Statistical Rigor**
- Define anomalies quantitatively (5th percentile threshold + zero detection)
- Use weighted composite scores reflecting business priorities
- Report both point estimates and distributions

### 3. **Hierarchical Analysis**
- National level: Identify which states need focus
- District level: Understand geographic clustering
- Pincode level: Pinpoint specific operational problem areas
- This hierarchy enables both strategic and tactical decision-making

### 4. **Accessibility to Decision-Makers**
- Completion rates convert raw counts to intuitive 0-1 scale
- Log-scale visualizations accommodate extreme value ranges
- Risk scores enable prioritization within resource constraints
- Visual outputs (histograms, charts) convey patterns immediately

### 5. **Actionability**
- Top 10 critical pincodes per state are immediately actionable: "Send resources to these specific locations"
- Risk score rankings enable sequencing: "Address highest-risk first"
- District-level summaries help identify systemic state-level issues

---

## Outputs Generated

This methodology produces four key output categories:

| Output | Level | Format | Use Case |
|--------|-------|--------|----------|
| **Critical Pincodes List** | Pincode | CSV | Operational resource allocation |
| **Risk State Summaries** | State-District-Pincode | CSV | Comprehensive problem mapping |
| **State Histograms** | State | PNG Images | Stakeholder briefings |
| **Statistical Summaries** | National & State | Console/CSV | Trend analysis and benchmarking |

These outputs together create a complete evidence base for decision-making: from national strategy down to specific operational interventions.

